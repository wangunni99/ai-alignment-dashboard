import streamlit as st
import pandas as pd
import numpy as np
import networkx as nx
from pyvis.network import Network
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import altair as alt
import io
import re

# SBERT ëª¨ë¸ ë¡œë”© (ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”)
# Streamlit í™˜ê²½ì—ì„œ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ st.cache_resource ì‚¬ìš©
@st.cache_resource
def load_sbert_model():
    try:
        # ì‚¬ìš©ìê°€ ìš”ì²­í•œ í•œêµ­ì–´ íŠ¹í™” SBERT ëª¨ë¸
        model_name = 'jhgan/ko-sbert-multitask'
        model = SentenceTransformer(model_name)
        return model
    except ImportError:
        st.error("ğŸš¨ **ì˜¤ë¥˜:** 'sentence-transformers' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `requirements.txt`ë¥¼ í™•ì¸í•˜ê³  ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        st.error(f"ğŸš¨ **ì˜¤ë¥˜:** SBERT ëª¨ë¸ ë¡œë”© ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
def load_and_preprocess_data(uploaded_file):
    """ì—‘ì…€ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    if uploaded_file is None:
        return None, None

    try:
        # openpyxl ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ì—‘ì…€ íŒŒì¼ ë¡œë“œ
        xls = pd.ExcelFile(uploaded_file, engine='openpyxl')
        
        # ì‹œíŠ¸ ë¡œë“œ
        df_business = pd.read_excel(xls, 'ì‚¬ì—…')
        df_tech = pd.read_excel(xls, 'ê¸°ìˆ ')

        # ë°ì´í„° ì •ê·œí™” ë° ê²°í•©
        df_business = preprocess_data(df_business, 'ì‚¬ì—…')
        df_tech = preprocess_data(df_tech, 'ê¸°ìˆ ')

        return df_business, df_tech

    except ValueError as e:
        st.error(f"ğŸš¨ **ì˜¤ë¥˜:** ì—‘ì…€ íŒŒì¼ì— 'ì‚¬ì—…' ë˜ëŠ” 'ê¸°ìˆ ' ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œíŠ¸ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”. ({e})")
        return None, None
    except Exception as e:
        st.error(f"ğŸš¨ **ì˜¤ë¥˜:** íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None

def preprocess_data(df, project_type):
    """ê°œë³„ ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ (ì¡°ì§ëª… ì •ê·œí™”, ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„±)"""
    
    # ì»¬ëŸ¼ëª… í†µì¼
    df.columns = [
        'í”„ë¡œì íŠ¸ëª…', 'ì„¤ëª…', 'PO ì¡°ì§', 'ìœ ê´€ ì¡°ì§'
    ]
    
    # ë°ì´í„°í”„ë ˆì„ì— í”„ë¡œì íŠ¸ëª…ê³¼ ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if 'í”„ë¡œì íŠ¸ëª…' not in df.columns or 'ì„¤ëª…' not in df.columns:
        st.error(f"ğŸš¨ **ì˜¤ë¥˜:** '{project_type}' ì‹œíŠ¸ì— 'í”„ë¡œì íŠ¸ëª…' ë˜ëŠ” 'ì„¤ëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    # ì¡°ì§ëª… ê²°ì¸¡ê°’ ì²˜ë¦¬: 'nan' ë…¸ë“œ ë°©ì§€ë¥¼ ìœ„í•´ 'ë¯¸ì§€ì •'ìœ¼ë¡œ ëŒ€ì²´
    df['PO ì¡°ì§'] = df['PO ì¡°ì§'].fillna('ë¯¸ì§€ì •')
    df['ìœ ê´€ ì¡°ì§'] = df['ìœ ê´€ ì¡°ì§'].fillna('') # ìœ ê´€ ì¡°ì§ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ë˜ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬

    # ì¡°ì§ëª… ì •ê·œí™”: ì‰¼í‘œ(,) ë˜ëŠ” ì¤„ë°”ê¿ˆ(\n)ìœ¼ë¡œ ë¶„ë¦¬ í›„ ê³µë°± ì œê±°
    def normalize_orgs(org_str):
        if pd.isna(org_str) or org_str == '':
            return []
        # ì‰¼í‘œ, ì¤„ë°”ê¿ˆ, ì„¸ë¯¸ì½œë¡  ë“±ì„ êµ¬ë¶„ìë¡œ ì‚¬ìš©
        org_list = re.split(r'[,\n;]', str(org_str))
        # ê° ì¡°ì§ëª…ì—ì„œ ì•ë’¤ ê³µë°± ì œê±°
        return [org.strip() for org in org_list if org.strip()]

    df['ìœ ê´€ ì¡°ì§_list'] = df['ìœ ê´€ ì¡°ì§'].apply(normalize_orgs)
    
    # ì„ë² ë”©ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ìƒì„±: 'í”„ë¡œì íŠ¸ëª… + ì„¤ëª…'
    df['embedding_text'] = df['í”„ë¡œì íŠ¸ëª…'].fillna('') + " [ì„¤ëª…]: " + df['ì„¤ëª…'].fillna('')
    
    # í”„ë¡œì íŠ¸ ê³ ìœ  ID ìƒì„±
    df['project_id'] = [f"{project_type}_{i}" for i in range(len(df))]
    df['project_type'] = project_type
    
    return df

# 2. í•µì‹¬ ë¶„ì„ ë¡œì§ (Backend)
def get_embeddings(texts, model):
    """SBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
    if model is None:
        return np.array([])
    
    with st.spinner("â³ í”„ë¡œì íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•˜ëŠ” ì¤‘... (SBERT ëª¨ë¸ ì‚¬ìš©)"):
        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ í•„í„°ë§
        valid_texts = [t for t in texts if t.strip()]
        if not valid_texts:
            return np.array([])
            
        embeddings = model.encode(valid_texts, convert_to_tensor=True)
    return embeddings.cpu().numpy()

def calculate_similarity(business_embeddings, tech_embeddings):
    """ì‚¬ì—… í”„ë¡œì íŠ¸ì™€ ê¸°ìˆ  í”„ë¡œì íŠ¸ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if business_embeddings.size == 0 or tech_embeddings.size == 0:
        return np.array([[]])
        
    with st.spinner("ğŸ“ í”„ë¡œì íŠ¸ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ì¤‘..."):
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = cosine_similarity(business_embeddings, tech_embeddings)
    return similarity_matrix

def get_matches(df_business, df_tech, similarity_matrix, threshold):
    """ìœ ì‚¬ë„ ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ëœ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    matches = []
    
    # ìœ ì‚¬ë„ í–‰ë ¬ì„ ìˆœíšŒí•˜ë©° ì„ê³„ê°’ ì´ìƒì˜ ë§¤ì¹­ì„ ì°¾ìŒ
    for i in range(similarity_matrix.shape[0]):
        for j in range(similarity_matrix.shape[1]):
            similarity = similarity_matrix[i, j]
            if similarity >= threshold:
                matches.append({
                    'ì‚¬ì—…_ID': df_business.iloc[i]['project_id'],
                    'ì‚¬ì—…_í”„ë¡œì íŠ¸ëª…': df_business.iloc[i]['í”„ë¡œì íŠ¸ëª…'],
                    'ì‚¬ì—…_PO_ì¡°ì§': df_business.iloc[i]['PO ì¡°ì§'],
                    'ê¸°ìˆ _ID': df_tech.iloc[j]['project_id'],
                    'ê¸°ìˆ _í”„ë¡œì íŠ¸ëª…': df_tech.iloc[j]['í”„ë¡œì íŠ¸ëª…'],
                    'ê¸°ìˆ _PO_ì¡°ì§': df_tech.iloc[j]['PO ì¡°ì§'],
                    'ìœ ì‚¬ë„': similarity
                })
    
    df_matches = pd.DataFrame(matches)
    return df_matches

# 3. ëŒ€ì‹œë³´ë“œ UI/UX êµ¬í˜„
def display_kpis(df_business, df_tech, df_matches):
    """ë©”ì¸ ìƒë‹¨ KPI ìš”ì•½ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    
    total_business = len(df_business)
    total_tech = len(df_tech)
    matched_count = len(df_matches)
    
    # ë§¤ì¹­ëœ ì‚¬ì—… í”„ë¡œì íŠ¸ ìˆ˜ (ì¤‘ë³µ ì œê±°)
    matched_business_count = df_matches['ì‚¬ì—…_ID'].nunique() if not df_matches.empty else 0
    
    # Alignment Rate (%) ê³„ì‚°
    if total_business > 0:
        alignment_rate = (matched_business_count / total_business) * 100
    else:
        alignment_rate = 0
        
    st.subheader("ğŸ“Š AI í”„ë¡œì íŠ¸ ì–¼ë¼ì¸ë¨¼íŠ¸ í˜„í™© ìš”ì•½")
    
    col1, col2, col3, col4 = st.columns(4)
    
    col1.metric("ì´ ì‚¬ì—… í”„ë¡œì íŠ¸ ìˆ˜", f"{total_business}ê±´")
    col2.metric("ì´ ê¸°ìˆ  í”„ë¡œì íŠ¸ ìˆ˜", f"{total_tech}ê±´")
    col3.metric("ë§¤ì¹­ëœ ì—°ê²° ê±´ìˆ˜", f"{matched_count}ê±´")
    col4.metric("Alignment Rate (%)", f"{alignment_rate:.1f}%", 
                help="ê¸°ìˆ  í”„ë¡œì íŠ¸ì™€ ë§¤ì¹­ëœ ì‚¬ì—… í”„ë¡œì íŠ¸ì˜ ë¹„ìœ¨")

def create_network_map(df_matches):
    """PyVisì™€ NetworkXë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°ì§ ê°„ í˜‘ì—… ë„¤íŠ¸ì›Œí¬ ë§µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # NetworkX ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    # ë…¸ë“œ ì¶”ê°€: PO ì¡°ì§
    all_orgs = pd.concat([df_matches['ì‚¬ì—…_PO_ì¡°ì§'], df_matches['ê¸°ìˆ _PO_ì¡°ì§']]).unique()
    for org in all_orgs:
        G.add_node(org, group='ì¡°ì§')

    # ì—£ì§€ ì¶”ê°€: ë§¤ì¹­ëœ í”„ë¡œì íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¡°ì§ ê°„ í˜‘ì—… ê´€ê³„ë¥¼ ì—£ì§€ë¡œ í‘œí˜„
    for index, row in df_matches.iterrows():
        org1 = row['ì‚¬ì—…_PO_ì¡°ì§']
        org2 = row['ê¸°ìˆ _PO_ì¡°ì§']
        similarity = row['ìœ ì‚¬ë„']
        
        # ì¡°ì§ì´ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ ì—£ì§€ ì¶”ê°€ (ìê¸° ìì‹ ê³¼ì˜ ì—°ê²° ì œì™¸)
        if org1 != org2:
            # ì—£ì§€ ê°€ì¤‘ì¹˜ (ìœ ì‚¬ë„)ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜‘ì—… ê°•ë„ í‘œí˜„
            if G.has_edge(org1, org2):
                # ì´ë¯¸ ì—£ì§€ê°€ ìˆë‹¤ë©´ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (í•©ì‚°)
                G[org1][org2]['title'] += f" - {row['ì‚¬ì—…_í”„ë¡œì íŠ¸ëª…']} - {row['ê¸°ìˆ _í”„ë¡œì íŠ¸ëª…']} ({similarity:.2f})"
                G[org1][org2]['weight'] += similarity
                G[org1][org2]['label'] = f"{G[org1][org2]['weight']:.1f}"
            else:
                G.add_edge(org1, org2, 
                           weight=similarity, 
                           title=f"- {row['ì‚¬ì—…_í”„ë¡œì íŠ¸ëª…']} - {row['ê¸°ìˆ _í”„ë¡œì íŠ¸ëª…']} ({similarity:.2f})",
                           label=f"{similarity:.1f}")

    # --- ì‹œê°í™” ê°œì„  ë¡œì§ ---
    
    # 1. ë…¸ë“œ í¬ê¸°: ì—°ê²° ì¤‘ì‹¬ì„±(Degree Centrality) ë°˜ì˜
    if G.number_of_nodes() > 0:
        # ì—°ê²° ì¤‘ì‹¬ì„± ê³„ì‚°
        degree_centrality = nx.degree_centrality(G)
        
        # ë…¸ë“œ í¬ê¸° ì—…ë°ì´íŠ¸: ì¤‘ì‹¬ì„±ì— ë¹„ë¡€í•˜ì—¬ í¬ê¸° ì„¤ì • (ìµœì†Œ 10, ìµœëŒ€ 50)
        max_centrality = max(degree_centrality.values()) if degree_centrality else 1
        for node in G.nodes():
            centrality = degree_centrality.get(node, 0)
            # í¬ê¸° ë³€í™” í­ì„ í¬ê²Œ ì„¤ì •
            size = 10 + (centrality / max_centrality) * 40 
            G.nodes[node]['size'] = size
            G.nodes[node]['title'] = f"ì¡°ì§: {node}  
ì—°ê²° ì¤‘ì‹¬ì„±: {centrality:.2f}  
ì´ í˜‘ì—… ê°•ë„: {G.degree(node, weight='weight'):.1f}"
    
    # 2. PyVis ë„¤íŠ¸ì›Œí¬ ìƒì„±
    net = Network(height="600px", width="100%", bgcolor="#222222", font_color="white", cdn_resources='local')
    
    # 3. ë¬¼ë¦¬ ì—”ì§„ ì„¤ì • ê°•í™” (êµ°ì§‘í™” ê°œì„ )
    net.set_options("""
    var options = {
      "physics": {
        "forceAtlas2Based": {
          "gravitationalConstant": -50,  // ì¸ë ¥ ê°•í™” (ë…¸ë“œë“¤ì´ ë” ì˜ ë­‰ì¹¨)
          "centralGravity": 0.01,
          "springLength": 150,
          "springConstant": 0.08
        },
        "minVelocity": 0.75,
        "solver": "forceAtlas2Based"
      },
      "edges": {
        "color": {
          "inherit": true
        },
        "smooth": {
          "enabled": true,
          "type": "dynamic"
        }
      }
    }
    """)
    
    # NetworkX ê·¸ë˜í”„ë¥¼ PyVisë¡œ ë³€í™˜ (ìˆ˜ë™ ë³€í™˜ìœ¼ë¡œ PyVis í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
    for node in G.nodes(data=True):
        node_id = node[0]
        node_data = node[1]
        net.add_node(str(node_id), 
                     label=str(node_id), 
                     title=node_data.get('title', str(node_id)), 
                     group=node_data.get('group'), 
                     size=node_data.get('size', 10))
    
    for edge in G.edges(data=True):
        net.add_edge(str(edge[0]), str(edge[1]), 
                     value=edge[2].get('weight'), 
                     title=edge[2].get('title'), 
                     label=edge[2].get('label'))
    
    # HTML íŒŒì¼ë¡œ ì €ì¥
    net.save_graph("network_map.html")
    
    # Streamlitì— HTML ë Œë”ë§
    try:
        import streamlit.components.v1 as components
        with open("network_map.html", 'r', encoding='utf-8') as f:
            html_content = f.read()
        components.html(html_content, height=650)
    except Exception as e:
        st.error(f"PyVis ë Œë”ë§ ì˜¤ë¥˜: {e}")
        st.info("PyVis ë„¤íŠ¸ì›Œí¬ ë§µì„ ë Œë”ë§í•˜ë ¤ë©´ `streamlit.components.v1`ì´ í•„ìš”í•©ë‹ˆë‹¤.")

def display_gap_analysis(df_business, df_tech, df_matches):
    """ê°­ ë¶„ì„ (Tech Gap, Tech Push) ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    
    # 1. Tech Gap: ê¸°ìˆ  ê³¼ì œì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì‚¬ì—… í”„ë¡œì íŠ¸
    matched_business_ids = set(df_matches['ì‚¬ì—…_ID']) if not df_matches.empty else set()
    df_tech_gap = df_business[~df_business['project_id'].isin(matched_business_ids)]
    
    st.markdown("#### ğŸ”´ Tech Gap (ê¸°ìˆ  ì§€ì› í•„ìš”)")
    st.info(f"ì´ {len(df_tech_gap)}ê±´ì˜ ì‚¬ì—… í”„ë¡œì íŠ¸ê°€ ë§¤ì¹­ë˜ëŠ” ê¸°ìˆ  í”„ë¡œì íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    if not df_tech_gap.empty:
        st.dataframe(df_tech_gap[['í”„ë¡œì íŠ¸ëª…', 'PO ì¡°ì§', 'ì„¤ëª…']], use_container_width=True)

    st.markdown("---")

    # 2. Tech Push: ì‚¬ì—… ê³¼ì œì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ê¸°ìˆ  í”„ë¡œì íŠ¸
    matched_tech_ids = set(df_matches['ê¸°ìˆ _ID']) if not df_matches.empty else set()
    df_tech_push = df_tech[~df_tech['project_id'].isin(matched_tech_ids)]
    
    st.markdown("#### ğŸŸ¢ Tech Push (ì‚¬ì—…í™” í•„ìš”)")
    st.info(f"ì´ {len(df_tech_push)}ê±´ì˜ ê¸°ìˆ  í”„ë¡œì íŠ¸ê°€ ë§¤ì¹­ë˜ëŠ” ì‚¬ì—… í”„ë¡œì íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    if not df_tech_push.empty:
        st.dataframe(df_tech_push[['í”„ë¡œì íŠ¸ëª…', 'PO ì¡°ì§', 'ì„¤ëª…']], use_container_width=True)

def display_workload(df_business, df_tech):
    """ì¡°ì§ë³„ í”„ë¡œì íŠ¸ í˜„í™© (Workload)ì„ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
    
    # PO ì¡°ì§ë³„ í”„ë¡œì íŠ¸ ìˆ˜ ì§‘ê³„
    df_business_po = df_business.groupby('PO ì¡°ì§').size().reset_index(name='ì‚¬ì—…_í”„ë¡œì íŠ¸_ìˆ˜')
    df_tech_po = df_tech.groupby('PO ì¡°ì§').size().reset_index(name='ê¸°ìˆ _í”„ë¡œì íŠ¸_ìˆ˜')
    
    # ë°ì´í„° ë³‘í•©
    df_workload = pd.merge(df_business_po, df_tech_po, on='PO ì¡°ì§', how='outer').fillna(0)
    
    # Wide formatì„ Long formatìœ¼ë¡œ ë³€í™˜ (Altair ì‹œê°í™”ë¥¼ ìœ„í•´)
    df_workload_long = pd.melt(df_workload, id_vars=['PO ì¡°ì§'], 
                               value_vars=['ì‚¬ì—…_í”„ë¡œì íŠ¸_ìˆ˜', 'ê¸°ìˆ _í”„ë¡œì íŠ¸_ìˆ˜'],
                               var_name='í”„ë¡œì íŠ¸_ìœ í˜•', value_name='í”„ë¡œì íŠ¸_ìˆ˜')
    
    st.markdown("#### ğŸ“ˆ ì¡°ì§ë³„ í”„ë¡œì íŠ¸ í˜„í™© (Workload)")
    
    # Altair ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±
    chart = alt.Chart(df_workload_long).mark_bar().encode(
        # xì¶•: í”„ë¡œì íŠ¸ ìˆ˜ (í•©ê³„)
        x=alt.X('í”„ë¡œì íŠ¸_ìˆ˜:Q', title='í”„ë¡œì íŠ¸ ìˆ˜ (í•©ê³„)'),
        # yì¶•: PO ì¡°ì§
        y=alt.Y('PO ì¡°ì§:N', sort='-x', title='PO ì¡°ì§'),
        # ìƒ‰ìƒ: í”„ë¡œì íŠ¸ ìœ í˜•ë³„ êµ¬ë¶„
        color=alt.Color('í”„ë¡œì íŠ¸_ìœ í˜•:N', title='ìœ í˜•'),
        # íˆ´íŒ ì„¤ì •
        tooltip=['PO ì¡°ì§', 'í”„ë¡œì íŠ¸_ìœ í˜•', 'í”„ë¡œì íŠ¸_ìˆ˜']
    ).properties(
        title="ì¡°ì§ë³„ ì‚¬ì—… ë° ê¸°ìˆ  í”„ë¡œì íŠ¸ ë‹´ë‹¹ í˜„í™©"
    ).interactive() # ì¤Œ/íŒ¬ ê¸°ëŠ¥ í™œì„±í™”
    
    st.altair_chart(chart, use_container_width=True)

def to_csv(df):
    """ë°ì´í„°í”„ë ˆì„ì„ CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ BOMì´ í¬í•¨ëœ UTF-8ë¡œ ì¸ì½”ë”©
    return df.to_csv(index=False, encoding='utf-8-sig')

def main():
    """Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    st.set_page_config(
        page_title="AI í”„ë¡œì íŠ¸ ì–¼ë¼ì¸ë¨¼íŠ¸ ëŒ€ì‹œë³´ë“œ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("ğŸ¤– AI í”„ë¡œì íŠ¸ ì–¼ë¼ì¸ë¨¼íŠ¸ ëŒ€ì‹œë³´ë“œ")
    st.markdown("ì‚¬ì—… í”„ë¡œì íŠ¸ì™€ ê¸°ìˆ  í”„ë¡œì íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì¡°ì§ ê°„ í˜‘ì—… ì‹œë„ˆì§€ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.")

    # --- ì‚¬ì´ë“œë°” (Settings) ---
    st.sidebar.header("âš™ï¸ ì„¤ì •")
    
    # ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
    uploaded_file = st.sidebar.file_uploader(
        "ì—‘ì…€ íŒŒì¼(.xlsx) ì—…ë¡œë“œ", 
        type=['xlsx'],
        help="ì‹œíŠ¸ 1: 'ì‚¬ì—…', ì‹œíŠ¸ 2: 'ê¸°ìˆ 'ì´ í¬í•¨ëœ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    )
    
    # ìœ ì‚¬ë„ ì„ê³„ê°’ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    threshold = st.sidebar.slider(
        "ìœ ì‚¬ë„ ì„ê³„ê°’ (Threshold)", 
        min_value=0.40, 
        max_value=0.95, 
        value=0.60, 
        step=0.01,
        help="ì´ ê°’ ì´ìƒì¸ í”„ë¡œì íŠ¸ë§Œ 'ì—°ê²°ëœ í”„ë¡œì íŠ¸'ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤."
    )
    
    # --- ë©”ì¸ ë¡œì§ ì‹¤í–‰ ---
    
    # ì—‘ì…€ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬
    df_business, df_tech = load_and_preprocess_data(uploaded_file)

    if df_business is None or df_tech is None or df_business.empty or df_tech.empty:
        st.info("â¬†ï¸ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
        return

    # SBERT ëª¨ë¸ ë¡œë“œ
    model = load_sbert_model()
    if model is None:
        return

    # í…ìŠ¤íŠ¸ ì„ë² ë”©
    business_embeddings = get_embeddings(df_business['embedding_text'].tolist(), model)
    tech_embeddings = get_embeddings(df_tech['embedding_text'].tolist(), model)
    
    if business_embeddings.size == 0 or tech_embeddings.size == 0:
        st.warning("ì„ë² ë”© ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë°ì´í„°(í”„ë¡œì íŠ¸ëª…, ì„¤ëª…)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = calculate_similarity(business_embeddings, tech_embeddings)
    
    # ë§¤ì¹­ ê²°ê³¼ ì¶”ì¶œ
    df_matches = get_matches(df_business, df_tech, similarity_matrix, threshold)

    # --- ë©”ì¸ ì½˜í…ì¸  ---
    
    # 1. KPI ìš”ì•½
    display_kpis(df_business, df_tech, df_matches)
    st.markdown("---")

    # 2. íƒ­ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸŒ ë„¤íŠ¸ì›Œí¬ ë§µ", "ğŸ” ê°­ ë¶„ì„", "ğŸ’¼ ë¦¬ì†ŒìŠ¤ í˜„í™©"])

    with tab1:
        st.header("ì¡°ì§ ê°„ í˜‘ì—… ë„¤íŠ¸ì›Œí¬ ë§µ")
        if df_matches.empty:
            st.warning("ë§¤ì¹­ëœ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë‚®ì¶”ê±°ë‚˜ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            create_network_map(df_matches)

    with tab2:
        st.header("í”„ë¡œì íŠ¸ ì–¼ë¼ì¸ë¨¼íŠ¸ ê°­ ë¶„ì„")
        display_gap_analysis(df_business, df_tech, df_matches)

    with tab3:
        st.header("ì¡°ì§ë³„ í”„ë¡œì íŠ¸ ë¦¬ì†ŒìŠ¤ í˜„í™©")
        display_workload(df_business, df_tech)

    # 3. ë§¤ì¹­ ê²°ê³¼ ìƒì„¸ í…Œì´ë¸” ë° ë‹¤ìš´ë¡œë“œ
    st.markdown("---")
    st.subheader("ğŸ“‹ ë§¤ì¹­ ê²°ê³¼ ìƒì„¸")
    
    if df_matches.empty:
        st.info("í˜„ì¬ ì„ê³„ê°’(%.2f)ì—ì„œëŠ” ë§¤ì¹­ëœ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤." % threshold)
    else:
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        df_matches_sorted = df_matches.sort_values(by='ìœ ì‚¬ë„', ascending=False).reset_index(drop=True)
        
        st.dataframe(df_matches_sorted, use_container_width=True)
        
        # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        csv_data = to_csv(df_matches_sorted)
        st.download_button(
            label="ë§¤ì¹­ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_data,
            file_name='project_alignment_matches.csv',
            mime='text/csv',
        )

if __name__ == "__main__":
    main()
